{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark==3.1.0\n",
      "  Using cached delta_spark-3.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pyspark<3.6.0,>=3.5.0 (from delta-spark==3.1.0)\n",
      "  Downloading pyspark-3.5.5.tar.gz (317.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.0.0 in /home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages (from delta-spark==3.1.0) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages (from importlib-metadata>=1.0.0->delta-spark==3.1.0) (3.20.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages (from pyspark<3.6.0,>=3.5.0->delta-spark==3.1.0) (0.10.9.7)\n",
      "Using cached delta_spark-3.1.0-py3-none-any.whl (21 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.5-py2.py3-none-any.whl size=317747859 sha256=22a03d8ac1691165ffbbc3ab7dcc1fb7ed653b0f966aefd66718b77e68b275df\n",
      "  Stored in directory: /home/longnv95/.cache/pip/wheels/9e/5b/b4/a3ac8d456edf8c52eb15f9eb357d961812d5f17bf203c54c18\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark, delta-spark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.3.2\n",
      "    Uninstalling pyspark-3.3.2:\n",
      "      Successfully uninstalled pyspark-3.3.2\n",
      "Successfully installed delta-spark-3.1.0 pyspark-3.5.5\n"
     ]
    }
   ],
   "source": [
    "# pip install binance-connector\n",
    "# pyspark==3.4.1\n",
    "# pydeequ==1.0.1\n",
    "# python-dotenv==1.0.0\n",
    "# nltk==3.8.1\n",
    "# apache-flink==1.17.1\n",
    "# kafka-python==2.0.2\n",
    "# pip install minio==7.1.16\n",
    "# pip install deltalake==0.10.2\n",
    "# pip install delta-spark==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = open(\"/home/longnv95/Coding/MLOPs/api_binance.txt\", \"r\").read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages (from requests) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Method 1: Using python-dotenv (most common approach)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()  # By default looks for .env in current directory\n",
    "# Or specify a path: load_dotenv('/path/to/your/.env')\n",
    "\n",
    "# Access environment variables\n",
    "db_password = os.getenv('POSTGRES_PASSWORD')\n",
    "minio_key = os.getenv('MINIO_ACCESS_KEY')\n",
    "fernet_key = os.getenv('AIRFLOW__CORE__FERNET_KEY')\n",
    "\n",
    "print(f\"DB Password: {db_password}\")\n",
    "print(f\"MinIO Key: {minio_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load API Key and run_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 00:00:00 1740502800000\n",
      "2025-02-26 23:00:00 1740585600000\n"
     ]
    }
   ],
   "source": [
    "from binance.spot import Spot\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Binance API Key (Public API, no need for secret key)\n",
    "client = Spot(api_key=API_KEY)  # No secret key needed for public market data\n",
    "\n",
    "# Define the time range for historical data (e.g. yesterday at noon)\n",
    "run_date_str = \"2025-02-26\"\n",
    "\n",
    "run_date = datetime.datetime.strptime(run_date_str, \"%Y-%m-%d\")\n",
    "run_date = datetime.datetime.combine(run_date, datetime.time(0, 0))\n",
    "start_time_ms = int(run_date.timestamp() * 1000)\n",
    "print(run_date, start_time_ms)\n",
    "\n",
    "end_date = run_date + datetime.timedelta(hours=23)\n",
    "end_time_ms = int(end_date.timestamp() * 1000)\n",
    "print(end_date, end_time_ms)\n",
    "\n",
    "# For a 24-hour period at 30-minute intervals, we expect 48 candles (24 hours / 0.5 hour)\n",
    "limit = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the result from Binance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BTCUSDT\n",
      "    symbols  lastPrice  highPrice  lowPrice   quoteVolume   count        date  \\\n",
      "0   BTCUSDT   87259.99   87805.40  86831.52  1.874481e+08  373402  2025-02-26   \n",
      "1   BTCUSDT   87070.07   87791.56  86918.41  1.479738e+08  355911  2025-02-26   \n",
      "2   BTCUSDT   87947.25   88065.45  86508.76  1.830466e+08  372140  2025-02-26   \n",
      "3   BTCUSDT   88161.03   89013.66  87622.64  3.546604e+08  442290  2025-02-26   \n",
      "4   BTCUSDT   88821.00   88833.33  87761.90  1.488578e+08  238117  2025-02-26   \n",
      "5   BTCUSDT   88807.64   89460.17  88512.34  1.339590e+08  193335  2025-02-26   \n",
      "6   BTCUSDT   88680.40   89139.75  88675.14  1.091807e+08  147539  2025-02-26   \n",
      "7   BTCUSDT   88150.00   88690.00  88148.48  8.916528e+07  152315  2025-02-26   \n",
      "8   BTCUSDT   88705.66   88850.07  88149.98  7.598015e+07  160293  2025-02-26   \n",
      "9   BTCUSDT   89018.87   89198.23  88434.78  7.083051e+07  153079  2025-02-26   \n",
      "10  BTCUSDT   88900.00   89414.15  88886.35  7.488482e+07  169791  2025-02-26   \n",
      "11  BTCUSDT   88693.95   89245.29  88673.11  8.000823e+07  137137  2025-02-26   \n",
      "12  BTCUSDT   88423.60   88890.00  88392.45  8.770804e+07  167555  2025-02-26   \n",
      "13  BTCUSDT   88920.18   88940.01  88422.79  8.157571e+07  139090  2025-02-26   \n",
      "14  BTCUSDT   88720.02   89163.39  88399.43  9.204779e+07  144718  2025-02-26   \n",
      "15  BTCUSDT   88484.54   88807.80  88138.00  9.422059e+07  174532  2025-02-26   \n",
      "16  BTCUSDT   89115.98   89289.67  88464.00  1.105778e+08  144366  2025-02-26   \n",
      "17  BTCUSDT   89262.66   89314.45  88882.35  7.983399e+07  116305  2025-02-26   \n",
      "18  BTCUSDT   88616.80   89270.00  88534.55  8.636653e+07  144335  2025-02-26   \n",
      "19  BTCUSDT   87938.81   88674.00  87616.24  1.318362e+08  223859  2025-02-26   \n",
      "20  BTCUSDT   87191.99   88090.90  87031.73  1.425204e+08  320980  2025-02-26   \n",
      "21  BTCUSDT   87621.22   87741.33  85418.72  5.652988e+08  909510  2025-02-26   \n",
      "22  BTCUSDT   87567.53   88530.00  87422.00  3.135889e+08  667155  2025-02-26   \n",
      "23  BTCUSDT   86805.99   87806.62  86280.82  2.605028e+08  602949  2025-02-26   \n",
      "\n",
      "    hour  \n",
      "0      0  \n",
      "1      1  \n",
      "2      2  \n",
      "3      3  \n",
      "4      4  \n",
      "5      5  \n",
      "6      6  \n",
      "7      7  \n",
      "8      8  \n",
      "9      9  \n",
      "10    10  \n",
      "11    11  \n",
      "12    12  \n",
      "13    13  \n",
      "14    14  \n",
      "15    15  \n",
      "16    16  \n",
      "17    17  \n",
      "18    18  \n",
      "19    19  \n",
      "20    20  \n",
      "21    21  \n",
      "22    22  \n",
      "23    23  \n",
      "Processing ETHUSDT\n",
      "    symbols  lastPrice  highPrice  lowPrice   quoteVolume   count        date  \\\n",
      "0   BTCUSDT   87259.99   87805.40  86831.52  1.874481e+08  373402  2025-02-26   \n",
      "1   BTCUSDT   87070.07   87791.56  86918.41  1.479738e+08  355911  2025-02-26   \n",
      "2   BTCUSDT   87947.25   88065.45  86508.76  1.830466e+08  372140  2025-02-26   \n",
      "3   BTCUSDT   88161.03   89013.66  87622.64  3.546604e+08  442290  2025-02-26   \n",
      "4   BTCUSDT   88821.00   88833.33  87761.90  1.488578e+08  238117  2025-02-26   \n",
      "5   BTCUSDT   88807.64   89460.17  88512.34  1.339590e+08  193335  2025-02-26   \n",
      "6   BTCUSDT   88680.40   89139.75  88675.14  1.091807e+08  147539  2025-02-26   \n",
      "7   BTCUSDT   88150.00   88690.00  88148.48  8.916528e+07  152315  2025-02-26   \n",
      "8   BTCUSDT   88705.66   88850.07  88149.98  7.598015e+07  160293  2025-02-26   \n",
      "9   BTCUSDT   89018.87   89198.23  88434.78  7.083051e+07  153079  2025-02-26   \n",
      "10  BTCUSDT   88900.00   89414.15  88886.35  7.488482e+07  169791  2025-02-26   \n",
      "11  BTCUSDT   88693.95   89245.29  88673.11  8.000823e+07  137137  2025-02-26   \n",
      "12  BTCUSDT   88423.60   88890.00  88392.45  8.770804e+07  167555  2025-02-26   \n",
      "13  BTCUSDT   88920.18   88940.01  88422.79  8.157571e+07  139090  2025-02-26   \n",
      "14  BTCUSDT   88720.02   89163.39  88399.43  9.204779e+07  144718  2025-02-26   \n",
      "15  BTCUSDT   88484.54   88807.80  88138.00  9.422059e+07  174532  2025-02-26   \n",
      "16  BTCUSDT   89115.98   89289.67  88464.00  1.105778e+08  144366  2025-02-26   \n",
      "17  BTCUSDT   89262.66   89314.45  88882.35  7.983399e+07  116305  2025-02-26   \n",
      "18  BTCUSDT   88616.80   89270.00  88534.55  8.636653e+07  144335  2025-02-26   \n",
      "19  BTCUSDT   87938.81   88674.00  87616.24  1.318362e+08  223859  2025-02-26   \n",
      "20  BTCUSDT   87191.99   88090.90  87031.73  1.425204e+08  320980  2025-02-26   \n",
      "21  BTCUSDT   87621.22   87741.33  85418.72  5.652988e+08  909510  2025-02-26   \n",
      "22  BTCUSDT   87567.53   88530.00  87422.00  3.135889e+08  667155  2025-02-26   \n",
      "23  BTCUSDT   86805.99   87806.62  86280.82  2.605028e+08  602949  2025-02-26   \n",
      "24  ETHUSDT    2421.19    2443.00   2411.23  7.098426e+07  221464  2025-02-26   \n",
      "25  ETHUSDT    2424.35    2435.90   2404.04  5.260101e+07  207926  2025-02-26   \n",
      "26  ETHUSDT    2471.91    2473.59   2411.78  8.788840e+07  236241  2025-02-26   \n",
      "27  ETHUSDT    2493.37    2516.87   2457.59  1.065112e+08  242510  2025-02-26   \n",
      "28  ETHUSDT    2513.83    2514.91   2481.65  4.686407e+07  123519  2025-02-26   \n",
      "29  ETHUSDT    2510.95    2533.49   2499.09  4.887354e+07  113383  2025-02-26   \n",
      "30  ETHUSDT    2495.70    2522.24   2494.17  3.174592e+07  104187  2025-02-26   \n",
      "31  ETHUSDT    2475.10    2499.67   2471.80  3.778047e+07  124906  2025-02-26   \n",
      "32  ETHUSDT    2486.34    2492.06   2475.00  3.646334e+07  121538  2025-02-26   \n",
      "33  ETHUSDT    2498.51    2504.68   2476.85  3.908253e+07  144980  2025-02-26   \n",
      "34  ETHUSDT    2490.60    2507.50   2488.40  4.341385e+07  137210  2025-02-26   \n",
      "35  ETHUSDT    2489.70    2501.50   2486.80  3.190065e+07  100595  2025-02-26   \n",
      "36  ETHUSDT    2475.48    2495.60   2475.37  3.237918e+07  102036  2025-02-26   \n",
      "37  ETHUSDT    2500.89    2501.66   2475.20  4.196755e+07  118832  2025-02-26   \n",
      "38  ETHUSDT    2483.81    2506.40   2476.85  5.354131e+07  133989  2025-02-26   \n",
      "39  ETHUSDT    2464.39    2487.99   2457.77  6.195551e+07  177602  2025-02-26   \n",
      "40  ETHUSDT    2490.25    2494.61   2463.00  5.309134e+07  148690  2025-02-26   \n",
      "41  ETHUSDT    2494.02    2495.79   2478.61  7.067921e+07   89115  2025-02-26   \n",
      "42  ETHUSDT    2464.79    2496.25   2463.14  4.659851e+07  114269  2025-02-26   \n",
      "43  ETHUSDT    2433.11    2466.81   2423.48  7.497411e+07  222224  2025-02-26   \n",
      "44  ETHUSDT    2416.80    2440.40   2410.28  5.633253e+07  218801  2025-02-26   \n",
      "45  ETHUSDT    2435.88    2436.80   2370.00  1.738028e+08  450362  2025-02-26   \n",
      "46  ETHUSDT    2428.47    2461.95   2424.50  9.783993e+07  302426  2025-02-26   \n",
      "47  ETHUSDT    2398.69    2434.20   2385.16  9.273580e+07  298085  2025-02-26   \n",
      "\n",
      "    hour  \n",
      "0      0  \n",
      "1      1  \n",
      "2      2  \n",
      "3      3  \n",
      "4      4  \n",
      "5      5  \n",
      "6      6  \n",
      "7      7  \n",
      "8      8  \n",
      "9      9  \n",
      "10    10  \n",
      "11    11  \n",
      "12    12  \n",
      "13    13  \n",
      "14    14  \n",
      "15    15  \n",
      "16    16  \n",
      "17    17  \n",
      "18    18  \n",
      "19    19  \n",
      "20    20  \n",
      "21    21  \n",
      "22    22  \n",
      "23    23  \n",
      "24     0  \n",
      "25     1  \n",
      "26     2  \n",
      "27     3  \n",
      "28     4  \n",
      "29     5  \n",
      "30     6  \n",
      "31     7  \n",
      "32     8  \n",
      "33     9  \n",
      "34    10  \n",
      "35    11  \n",
      "36    12  \n",
      "37    13  \n",
      "38    14  \n",
      "39    15  \n",
      "40    16  \n",
      "41    17  \n",
      "42    18  \n",
      "43    19  \n",
      "44    20  \n",
      "45    21  \n",
      "46    22  \n",
      "47    23  \n"
     ]
    }
   ],
   "source": [
    "# Retrieve exchange info to get a list of all trading symbols\n",
    "exchange_info = client.exchange_info()\n",
    "symbols = [s['symbol'] for s in exchange_info['symbols'] if s['symbol'].lower().endswith('usdt')]\n",
    "\n",
    "len(symbols)\n",
    "\n",
    "# List to collect aggregated data for each symbol\n",
    "data = []\n",
    "\n",
    "for symbol in symbols[0:2]:\n",
    "    print(f\"Processing {symbol}\")\n",
    "    try:\n",
    "        # Fetch klines data for the given symbol and day\n",
    "        klines = client.klines(symbol=symbol, interval=\"1h\", \n",
    "                                startTime=start_time_ms, \n",
    "                                endTime=end_time_ms,\n",
    "                                limit=limit)\n",
    "        if not klines:\n",
    "            print(f\"No data for {symbol}\")\n",
    "            continue\n",
    "        \n",
    "        for candle in klines:\n",
    "            # Calculate aggregated metrics from the klines\n",
    "            lastPrice = float(candle[4])  # Last candle's close price\n",
    "            highPrice = float(candle[2])  # Highest high price across candles\n",
    "            lowPrice = float(candle[3])  # Lowest low price across candles\n",
    "            quoteVolume = float(candle[7]) # Sum of quote asset volumes\n",
    "            count = int(candle[8])           # Total number of trades\n",
    "        \n",
    "            # Use the provided run date and extract hour from the close time of the last candle\n",
    "            close_time_ms = int(candle[6])\n",
    "            close_time = datetime.datetime.fromtimestamp(close_time_ms / 1000)\n",
    "            hour = close_time.hour\n",
    "\n",
    "            # Append the aggregated data to our list\n",
    "            data.append({\n",
    "                \"symbols\": symbol,\n",
    "                \"lastPrice\": lastPrice,\n",
    "                \"highPrice\": highPrice,\n",
    "                \"lowPrice\": lowPrice,\n",
    "                \"quoteVolume\": quoteVolume,\n",
    "                \"count\": count,\n",
    "                \"date\": close_time.strftime(\"%Y-%m-%d\"),\n",
    "                \"hour\": close_time.hour\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {symbol}: {e}\")\n",
    "\n",
    "    # Create the DataFrame with the desired columns\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from binance.spot import Spot\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "API_KEY = open(\"/home/longnv95/Coding/MLOPs/api_binance.txt\", \"r\").read().strip()\n",
    "RUN_DATE_STR = \"2025-03-04\"\n",
    "\n",
    "client = Spot(api_key=API_KEY)\n",
    "\n",
    "def get_data_binance(symbol, start_date_str):\n",
    "    start_date = datetime.datetime.strptime(start_date_str, \"%Y-%m-%d\") \n",
    "    end_date = start_date + datetime.timedelta(hours=23)\n",
    "\n",
    "    start_time_ms = int(start_date.timestamp() * 1000)\n",
    "    end_time_ms = int(end_date.timestamp() * 1000)\n",
    "    \n",
    "    data = client.klines(symbol=symbol, interval=\"1h\", startTime=start_time_ms, endTime=end_time_ms)\n",
    "    df = pd.DataFrame(data, columns=[\"timestamp\", \"open\", \"high\", \n",
    "                                     \"low\", \"close\", \"volume\", \"close_time\", \n",
    "                                     \"quote_av\", \"trades\", \"tb_base_av\", \"tb_quote_av\", \"ignore\"])\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\") \n",
    "    df[\"close_time\"] = pd.to_datetime(df[\"close_time\"], unit=\"ms\")\n",
    "    df[\"symbol\"] = symbol\n",
    "    df[\"hour\"] = df['close_time'].apply(lambda x: x.hour)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_info = client.exchange_info()\n",
    "symbols = [s['symbol'] for s in exchange_info['symbols'] if s['symbol'].lower().endswith('usdt')][:2]\n",
    "test = get_data_binance('BTCUSDT', RUN_DATE_STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"./data_daily/spot_{run_date_str.replace('-','')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Limit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders Placed (Last Minute): None\n",
      "Request Weights Used (Last Minute): 6\n",
      "Order Weight Used (Last Minute): None\n"
     ]
    }
   ],
   "source": [
    "from binance.spot import Spot\n",
    "import requests\n",
    "import json\n",
    "\n",
    "client = Spot(api_key=API_KEY)\n",
    "\n",
    "try:\n",
    "    # Make a  API call\n",
    "    # Use the requests library to get headers\n",
    "    symbol = \"BTCUSDT\"\n",
    "    interval = \"1m\"  # 1-minute klines\n",
    "    limit = 10  # Number of klines to retrieve (adjust as needed)\n",
    "\n",
    "    url = \"https://api.binance.com/api/v3/klines\"  # Binance Kline API endpoint\n",
    "    params = {'symbol': symbol, 'interval': interval, 'limit': limit}\n",
    "    headers = {'X-MBX-APIKEY': API_KEY}  # Include the API key in the header\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "\n",
    "    # Access the headers from the response object\n",
    "    headers = response.headers\n",
    "\n",
    "    # Extract rate limit information\n",
    "    # These headers are the key to understanding your limits\n",
    "    order_count = headers.get('X-MBX-ORDER-COUNT-1M')\n",
    "    used_weight = headers.get('X-MBX-USED-WEIGHT-1M')\n",
    "    order_weight = headers.get('X-MBX-ORDER-WEIGHT-1M')\n",
    "\n",
    "    print(f\"Orders Placed (Last Minute): {order_count}\")\n",
    "    print(f\"Request Weights Used (Last Minute): {used_weight}\")\n",
    "    print(f\"Order Weight Used (Last Minute): {order_weight}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking rate limits: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User ID  Transaction ID      Amount      Vendor          Sources  \\\n",
      "0  user_000069    294063303192  579.803149   Education  Current Account   \n",
      "1  user_000000    413456285542  128.222111  Restaurant  Current Account   \n",
      "2  user_000023    168523878031  508.797821   Education      Credit Card   \n",
      "3  user_000095    160894934119  381.033603      Travel       Debit Card   \n",
      "4  user_000043    327308630118  611.396373   Education  Current Account   \n",
      "\n",
      "                  Time  \n",
      "0  2025-03-09 06:55:22  \n",
      "1  2025-09-03 22:19:31  \n",
      "2  2025-05-29 14:11:48  \n",
      "3  2025-03-20 23:05:23  \n",
      "4  2025-03-29 20:35:56  \n"
     ]
    }
   ],
   "source": [
    "RUN_DATE_STR = \"2025-03-04\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_pseudo_data(RUN_DATE_STR, num_rows=10000, num_users=1000):\n",
    "    \"\"\"Generates a Pandas DataFrame with pseudo transaction data.\"\"\"\n",
    "\n",
    "    # User IDs\n",
    "    user_ids = [f\"user_{i:06d}\" for i in np.random.choice(num_users, size=num_rows)]\n",
    "\n",
    "    # Transaction IDs\n",
    "    transaction_ids = np.random.randint(10**11, 10**12 - 1, size=num_rows)\n",
    "\n",
    "    # Source\n",
    "    sources = np.random.choice([\"Current Account\", \"Credit Card\", \"Debit Card\"], \n",
    "                               size=num_rows, p=[0.6, 0.3, 0.1])\n",
    "\n",
    "    # Amounts\n",
    "    amounts = np.random.uniform(1.0, 1000.0, size=num_rows)\n",
    "\n",
    "    # Vendors\n",
    "    vendors = [\"Online Shopping\", \"Hospital\", \"Sport\", \"Grocery\", \"Restaurant\",\n",
    "               \"Travel\", \"Entertainment\", \"Electronics\", \"Home Improvement\",\n",
    "               \"Clothing\", \"Education\", \"Sending Out\", \"Utilities\", \"Other\"]\n",
    "    vendor_probabilities = np.random.dirichlet(np.ones(len(vendors)))\n",
    "    vendor_choices = np.random.choice(vendors, size=num_rows, p=vendor_probabilities)\n",
    "\n",
    "    # Times\n",
    "    start_date = datetime.strptime(RUN_DATE_STR, \"%Y-%m-%d\")\n",
    "    time_deltas = [timedelta(seconds=np.random.randint(0, 31536000)) for _ in range(num_rows)]  # Up to 1 year\n",
    "    times = [(start_date + delta).strftime('%Y-%m-%d %H:%M:%S') for delta in time_deltas]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"User ID\": user_ids,\n",
    "        \"Transaction ID\": transaction_ids,\n",
    "        \"Amount\": amounts,\n",
    "        \"Vendor\": vendor_choices,\n",
    "        \"Sources\": sources,\n",
    "        \"Time\": times\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Generate and print the data\n",
    "pseudo_df = generate_pseudo_data(RUN_DATE_STR, num_rows=10000, num_users=100)\n",
    "print(pseudo_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/longnv95/Coding/MLOPs/final_project/data_daily/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()+'/data_daily/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Vendor</th>\n",
       "      <th>Sources</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_000028</td>\n",
       "      <td>502338847571</td>\n",
       "      <td>244.895708</td>\n",
       "      <td>Home Improvement</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>2025-09-29 22:19:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000727</td>\n",
       "      <td>479508446004</td>\n",
       "      <td>466.927997</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>Current Account</td>\n",
       "      <td>2025-03-21 22:14:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000547</td>\n",
       "      <td>113555903812</td>\n",
       "      <td>971.725657</td>\n",
       "      <td>Home Improvement</td>\n",
       "      <td>Current Account</td>\n",
       "      <td>2025-07-11 22:30:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000028</td>\n",
       "      <td>864048690465</td>\n",
       "      <td>417.963380</td>\n",
       "      <td>Travel</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2025-01-30 06:28:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000323</td>\n",
       "      <td>917408956397</td>\n",
       "      <td>555.758447</td>\n",
       "      <td>Sending Out</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2025-04-05 20:19:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User ID  Transaction ID      Amount            Vendor          Sources  \\\n",
       "0  user_000028    502338847571  244.895708  Home Improvement       Debit Card   \n",
       "1  user_000727    479508446004  466.927997        Restaurant  Current Account   \n",
       "2  user_000547    113555903812  971.725657  Home Improvement  Current Account   \n",
       "3  user_000028    864048690465  417.963380            Travel      Credit Card   \n",
       "4  user_000323    917408956397  555.758447       Sending Out      Credit Card   \n",
       "\n",
       "                  Time  \n",
       "0  2025-09-29 22:19:21  \n",
       "1  2025-03-21 22:14:40  \n",
       "2  2025-07-11 22:30:57  \n",
       "3  2025-01-30 06:28:11  \n",
       "4  2025-04-05 20:19:02  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('/home/longnv95/Coding/MLOPs/final_project/data_daily/transaction_2024-12-31.parquet').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id gender location occupation   day_start\n",
      "0  user_002562      M   City D      Other  2022-10-18\n",
      "1  user_002095      F   City B          F  2025-01-15\n",
      "2  user_003486      M   City C          F  2021-01-24\n",
      "3  user_004358      F   City A          F  2024-04-21\n",
      "4  user_003419      F   City B          M  2022-06-25\n"
     ]
    }
   ],
   "source": [
    "RUN_DATE_STR = \"2025-03-04\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_user_data(RUN_DATE_STR, num_rows=10000):\n",
    "    \"\"\"Generates a Pandas DataFrame with pseudo transaction data.\"\"\"\n",
    "\n",
    "    # User IDs\n",
    "    user_ids = [f\"user_{i:06d}\" for i in np.random.choice(num_rows, size=num_rows)]\n",
    "\n",
    "    # Transaction IDs\n",
    "    age = np.random.randint(22, 90, size=num_rows)\n",
    "\n",
    "    # Source\n",
    "    gender = np.random.choice([\"M\", \"F\", \"Other\"], \n",
    "                               size=num_rows, p=[0.5, 0.4, 0.1])\n",
    "\n",
    "    # Amounts\n",
    "    occupation = np.random.choice([\"M\", \"F\", \"Other\"], \n",
    "                               size=num_rows, p=[0.5, 0.4, 0.1])\n",
    "\n",
    "    # location\n",
    "    location = np.random.choice([\"City A\", \"City B\", \"City C\", \"City D\", \"City E\", \n",
    "                                 \"City F\", \"City G\", \"City H\", \"Unknown\"], \n",
    "                               size=num_rows, p=[0.1,0.12,0.08,0.1,0.15,0.05,0.1,0.1,0.2])\n",
    "    \n",
    "    # Day start\n",
    "    start = datetime(2020, 1, 1)\n",
    "    end = datetime(2025, 3, 1)\n",
    "    \n",
    "    # Calculate total days between start and end\n",
    "    days_between = (end - start).days\n",
    "    \n",
    "    # Generate random days and add to start date\n",
    "    random_days = np.random.randint(0, days_between, size=num_rows)\n",
    "    day_start = [(start + timedelta(days=int(x))).strftime('%Y-%m-%d') for x in random_days]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"user_id\": user_ids,\n",
    "        \"gender\": gender,\n",
    "        \"age\": age,\n",
    "        \"location\": location,\n",
    "        \"occupation\": occupation,\n",
    "        \"day_start\": day_start,\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Generate and print the data\n",
    "pseudo_df = generate_user_data(RUN_DATE_STR, num_rows=5000)\n",
    "print(pseudo_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = os.path.join(\"./scripts/\", '.env')\n",
    "load_dotenv(dotenv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/longnv95/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741367937.707676 1093992 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/07 17:19:05 WARN Utils: Your hostname, LongNV resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/03/07 17:19:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/07 17:19:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Data\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "# Columns\n",
    "columns = [\"language\",\"users_count\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data).toDF(*columns)\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+\n",
      "|language|users_count|subtract|\n",
      "+--------+-----------+--------+\n",
      "|    Java|      20000| 19000.0|\n",
      "|  Python|     100000| 99000.0|\n",
      "|   Scala|       3000|  2000.0|\n",
      "+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"subtract\", df[\"users_count\"] - 1000)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest data to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent message: {'symbol': 'BTCUSDT', 'lastPrice': 23000.0, 'highPrice': 23200.0, 'lowPrice': 22900.0, 'quoteVolume': 500000.0, 'count': 1500, 'date': '2025-02-26', 'hour': 0}\n",
      "Sent message: {'symbol': 'ETHUSDT', 'lastPrice': 1800.0, 'highPrice': 1820.0, 'lowPrice': 1785.0, 'quoteVolume': 300000.0, 'count': 900, 'date': '2025-02-26', 'hour': 0}\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# from kafka import KafkaProducer\n",
    "\n",
    "# # Example DataFrame (imagine this is populated with Binance data)\n",
    "# data = {\n",
    "#     \"symbol\": [\"BTCUSDT\", \"ETHUSDT\"],\n",
    "#     \"lastPrice\": [23000.0, 1800.0],\n",
    "#     \"highPrice\": [23200.0, 1820.0],\n",
    "#     \"lowPrice\": [22900.0, 1785.0],\n",
    "#     \"quoteVolume\": [500000.0, 300000.0],\n",
    "#     \"count\": [1500, 900],\n",
    "#     \"date\": [\"2025-02-26\", \"2025-02-26\"],\n",
    "#     \"hour\": [0, 0]\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Initialize the Kafka Producer (adjust bootstrap_servers as needed)\n",
    "# producer = KafkaProducer(\n",
    "#     bootstrap_servers=['localhost:9092'],\n",
    "#     value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "# )\n",
    "\n",
    "# topic = \"crypto_data\"\n",
    "\n",
    "# # Push each DataFrame row to Kafka as a JSON message\n",
    "# for _, row in df.iterrows():\n",
    "#     # Convert the row to a dictionary\n",
    "#     message = row.to_dict()\n",
    "#     producer.send(topic, message)\n",
    "#     print(\"Sent message:\", message)\n",
    "\n",
    "# # Flush to ensure all messages are delivered\n",
    "# producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 'crypto_data' created with retention.ms set to 172800000 (2 days)\n",
      "Data inserted into Kafka topic successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create retention policy with Kafka \n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Step 1: Create a Kafka Admin Client to manage topics\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=['localhost:9092'],  # Adjust as needed\n",
    "    client_id='admin'\n",
    ")\n",
    "\n",
    "# Define the topic name\n",
    "topic = \"crypto_data\"\n",
    "\n",
    "# Define retention period for 2 days in milliseconds (2 days = 2 * 24 * 60 * 60 * 1000)\n",
    "retention_ms = str(2 * 24 * 60 * 60 * 1000)  # \"172800000\"\n",
    "\n",
    "# Create a new topic with the specified retention policy\n",
    "new_topic = NewTopic(\n",
    "    name=topic,\n",
    "    num_partitions=1, # Modify this if you have multiple nodes\n",
    "    replication_factor=1, # Modify this if you have multiple nodes\n",
    "    topic_configs={\"retention.ms\": retention_ms}\n",
    ")\n",
    "\n",
    "# Try to create the topic (if it already exists, you'll get an exception)\n",
    "try:\n",
    "    admin_client.create_topics(new_topics=[new_topic], validate_only=False)\n",
    "    print(f\"Topic '{topic}' created with retention.ms set to {retention_ms} (2 days)\")\n",
    "except Exception as e:\n",
    "    print(f\"Topic creation issue (may already exist): {e}\")\n",
    "\n",
    "# Step 2: Create a Kafka Producer to send messages\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],  # Adjust with your broker addresses\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')  # Serialize messages as JSON\n",
    ")\n",
    "\n",
    "# Example data to send (this could be a row from your DataFrame, for instance)\n",
    "data = {\n",
    "    \"symbols\": \"BTCUSDT\",\n",
    "    \"lastPrice\": 20000.0,\n",
    "    \"highPrice\": 21000.0,\n",
    "    \"lowPrice\": 19500.0,\n",
    "    \"quoteVolume\": 500000.0,\n",
    "    \"count\": 1500,\n",
    "    \"date\": \"2025-02-26\",\n",
    "    \"hour\": 14\n",
    "}\n",
    "\n",
    "# Send the data to the Kafka topic\n",
    "producer.send(topic, data)\n",
    "producer.flush()  # Ensure all buffered messages are sent\n",
    "\n",
    "print(\"Data inserted into Kafka topic successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted topic: crypto_data\n"
     ]
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n",
    "\n",
    "def delete_topic(bootstrap_servers='localhost:9092', topic_name='topic_name'):\n",
    "    \"\"\"\n",
    "    Delete a Kafka topic\n",
    "    Args:\n",
    "        bootstrap_servers: Kafka bootstrap servers (default: 'localhost:9092')\n",
    "        topic_name: Name of topic to delete (default: 'topic_name')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create admin client\n",
    "        admin_client = KafkaAdminClient(\n",
    "            bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "        \n",
    "        # Delete topic\n",
    "        admin_client.delete_topics(topics=[topic_name])\n",
    "        print(f\"Successfully deleted topic: {topic_name}\")\n",
    "        \n",
    "    except UnknownTopicOrPartitionError:\n",
    "        print(f\"Topic {topic_name} does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete topic {topic_name}. Error: {e}\")\n",
    "    finally:\n",
    "        admin_client.close()\n",
    "\n",
    "# Usage example\n",
    "# if __name__ == \"__main__\":\n",
    "    # delete_topic(topic_name=\"crypto_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Binace Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_test = pd.read_csv(\"./data_daily/binance_data_2024-03-04.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. \n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 52 messages\n",
      "INFO:__main__:Successfully sent 8952 messages to topic crypto_data\n",
      "INFO:kafka.producer.kafka:Closing the Kafka producer with 9223372036.0 secs timeout.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1 host=localhost:9092 <connected> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. \n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "import logging\n",
    "import json\n",
    "from kafka import KafkaProducer \n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def send_message_batch(producer, topic_name: str, messages: list, batch_size: int = 100):\n",
    "    \"\"\"Send a batch of messages to Kafka\"\"\"\n",
    "    futures = []\n",
    "    \n",
    "    try:\n",
    "        for i in range(0, len(messages), batch_size):\n",
    "            batch = messages[i:i + batch_size]\n",
    "            \n",
    "            # Send batch of messages\n",
    "            for message in batch:\n",
    "                future = producer.send(topic_name, message)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # Wait for current batch to complete\n",
    "            producer.flush()\n",
    "            \n",
    "            # Check for any errors in the batch\n",
    "            for future in futures:\n",
    "                try:\n",
    "                    record_metadata = future.get(timeout=10)\n",
    "                    logger.debug(f\"Message sent to {record_metadata.topic}:{record_metadata.partition}:{record_metadata.offset}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to send message: {e}\")\n",
    "            \n",
    "            logger.info(f\"Processed batch of {len(batch)} messages\")\n",
    "            futures = []  # Clear futures for next batch\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in batch processing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Modified ingest function\n",
    "def ingest_data_to_kafka(df, topic_name: str, bootstrap_servers='localhost:9092', batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Ingest DataFrame to Kafka with batching and error handling\n",
    "    Args:\n",
    "        df: DataFrame to ingest\n",
    "        topic_name: Kafka topic name\n",
    "        bootstrap_servers: Kafka bootstrap servers\n",
    "        batch_size: Number of messages per batch\n",
    "    \"\"\"\n",
    "    # Create a Kafka Admin Client to manage topics\n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=['localhost:9092'],  # Adjust as needed\n",
    "        client_id='admin'\n",
    "    )\n",
    "\n",
    "    # Define retention period for 2 days in milliseconds (2 days = 2 * 24 * 60 * 60 * 1000)\n",
    "    retention_ms = str(2 * 24 * 60 * 60 * 1000)  # \"172800000\"\n",
    "\n",
    "    # Create a new topic with the specified retention policy\n",
    "    new_topic = NewTopic(\n",
    "        name=topic_name,\n",
    "        num_partitions=1, # Modify this if you have multiple nodes\n",
    "        replication_factor=1, # Modify this if you have multiple nodes\n",
    "        topic_configs={\"retention.ms\": retention_ms}\n",
    "    )\n",
    "\n",
    "    # Try to create the topic (if it already exists, you'll get an exception)\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=[new_topic], validate_only=False)\n",
    "        print(f\"Topic '{topic}' created with retention.ms set to {retention_ms} (2 days)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Topic creation issue (may already exist): {e}\")\n",
    "\n",
    "    # Ingest data to Kafka\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[bootstrap_servers],\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "        acks='all',\n",
    "        retries=3,\n",
    "        batch_size=16384,\n",
    "        linger_ms=100,\n",
    "        compression_type='gzip'  # Add compression\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Convert DataFrame to list of dictionaries more efficiently\n",
    "        messages = df.to_dict('records')\n",
    "        \n",
    "        # Send messages in batches\n",
    "        send_message_batch(producer, topic_name, messages, batch_size)\n",
    "        \n",
    "        logger.info(f\"Successfully sent {len(messages)} messages to topic {topic_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to ingest data: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        producer.close()\n",
    "\n",
    "# Usage example:\n",
    "ingest_data_to_kafka(df_test, \"crypto_data\", batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data with Flink (Not Use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_time</th>\n",
       "      <th>quote_av</th>\n",
       "      <th>trades</th>\n",
       "      <th>tb_base_av</th>\n",
       "      <th>tb_quote_av</th>\n",
       "      <th>ignore</th>\n",
       "      <th>symbol</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-03 17:00:00</td>\n",
       "      <td>62845.14</td>\n",
       "      <td>62989.00</td>\n",
       "      <td>62458.01</td>\n",
       "      <td>62570.01</td>\n",
       "      <td>1725.46165</td>\n",
       "      <td>2024-03-03 17:59:59.999</td>\n",
       "      <td>1.082620e+08</td>\n",
       "      <td>90105</td>\n",
       "      <td>891.18899</td>\n",
       "      <td>5.592146e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-03 18:00:00</td>\n",
       "      <td>62570.01</td>\n",
       "      <td>62863.68</td>\n",
       "      <td>62570.00</td>\n",
       "      <td>62811.10</td>\n",
       "      <td>957.69137</td>\n",
       "      <td>2024-03-03 18:59:59.999</td>\n",
       "      <td>6.007572e+07</td>\n",
       "      <td>60591</td>\n",
       "      <td>501.56021</td>\n",
       "      <td>3.146042e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-03 19:00:00</td>\n",
       "      <td>62811.10</td>\n",
       "      <td>62857.00</td>\n",
       "      <td>62653.89</td>\n",
       "      <td>62730.00</td>\n",
       "      <td>814.77449</td>\n",
       "      <td>2024-03-03 19:59:59.999</td>\n",
       "      <td>5.112155e+07</td>\n",
       "      <td>59993</td>\n",
       "      <td>384.98781</td>\n",
       "      <td>2.415434e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-03 20:00:00</td>\n",
       "      <td>62730.00</td>\n",
       "      <td>62859.99</td>\n",
       "      <td>62580.00</td>\n",
       "      <td>62757.99</td>\n",
       "      <td>843.07635</td>\n",
       "      <td>2024-03-03 20:59:59.999</td>\n",
       "      <td>5.288540e+07</td>\n",
       "      <td>51386</td>\n",
       "      <td>395.49832</td>\n",
       "      <td>2.480857e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-03 21:00:00</td>\n",
       "      <td>62758.00</td>\n",
       "      <td>62828.18</td>\n",
       "      <td>62623.76</td>\n",
       "      <td>62827.11</td>\n",
       "      <td>652.28143</td>\n",
       "      <td>2024-03-03 21:59:59.999</td>\n",
       "      <td>4.092035e+07</td>\n",
       "      <td>43335</td>\n",
       "      <td>319.10493</td>\n",
       "      <td>2.001938e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp      open      high       low     close      volume  \\\n",
       "0  2024-03-03 17:00:00  62845.14  62989.00  62458.01  62570.01  1725.46165   \n",
       "1  2024-03-03 18:00:00  62570.01  62863.68  62570.00  62811.10   957.69137   \n",
       "2  2024-03-03 19:00:00  62811.10  62857.00  62653.89  62730.00   814.77449   \n",
       "3  2024-03-03 20:00:00  62730.00  62859.99  62580.00  62757.99   843.07635   \n",
       "4  2024-03-03 21:00:00  62758.00  62828.18  62623.76  62827.11   652.28143   \n",
       "\n",
       "                close_time      quote_av  trades  tb_base_av   tb_quote_av  \\\n",
       "0  2024-03-03 17:59:59.999  1.082620e+08   90105   891.18899  5.592146e+07   \n",
       "1  2024-03-03 18:59:59.999  6.007572e+07   60591   501.56021  3.146042e+07   \n",
       "2  2024-03-03 19:59:59.999  5.112155e+07   59993   384.98781  2.415434e+07   \n",
       "3  2024-03-03 20:59:59.999  5.288540e+07   51386   395.49832  2.480857e+07   \n",
       "4  2024-03-03 21:59:59.999  4.092035e+07   43335   319.10493  2.001938e+07   \n",
       "\n",
       "   ignore   symbol  hour  \n",
       "0       0  BTCUSDT    17  \n",
       "1       0  BTCUSDT    18  \n",
       "2       0  BTCUSDT    19  \n",
       "3       0  BTCUSDT    20  \n",
       "4       0  BTCUSDT    21  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(\"./data_daily/binance_data_2024-03-04.csv\")   \n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time',\n",
       "       'quote_av', 'trades', 'tb_base_av', 'tb_quote_av', 'ignore', 'symbol',\n",
       "       'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import kafka\n",
    "print(kafka.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from Kafka and write down to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/15 19:23:38 WARN Utils: Your hostname, LongNV resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/03/15 19:23:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/longnv95/.ivy2/cache\n",
      "The jars for the packages stored in: /home/longnv95/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2f2ca241-a581-402b-8202-e6c5735171b4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.261 in central\n",
      ":: resolution report :: resolve 2036ms :: artifacts dl 210ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.261 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 by [com.amazonaws#aws-java-sdk-bundle;1.12.261] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   1   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2f2ca241-a581-402b-8202-e6c5735171b4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/45ms)\n",
      "25/03/15 19:23:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/15 19:24:08 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o66.showString.\n: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)\n\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)\n\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:128)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:374)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:67)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:345)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:476)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:162)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:175)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:175)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:168)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:221)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:266)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:235)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Convert the string date (agg_date) to a proper date type (specify the format if needed)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m json_df \u001b[38;5;241m=\u001b[39m json_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mto_date(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 48\u001b[0m \u001b[43mjson_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o66.showString.\n: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)\n\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)\n\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:128)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:374)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:67)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:345)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:476)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:162)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:175)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:175)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:168)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:221)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:266)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:235)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Create Spark session configured for MinIO (S3A)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weekly_Monthly_Feature_Calculation\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\" # previosly 2.4.0\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.2,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.261\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio_access_key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio_secret_key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read streaming data from Kafka topic \"crypto_data\"\n",
    "raw_kafka_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"crypto_data_agg_test\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# Define the schema for your JSON data (adjust field names and types as needed)\n",
    "schema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"price_daily\", DoubleType(), True),\n",
    "    StructField(\"volume_daily\", DoubleType(), True),\n",
    "    StructField(\"trades_daily\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Parse the JSON from the Kafka \"value\" column (which is binary)\n",
    "json_df = raw_kafka_df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Convert the string date (agg_date) to a proper date type (specify the format if needed)\n",
    "json_df = json_df.withColumn(\"date\", F.to_date(F.col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "json_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import load_cfg\n",
    "from minio import Minio\n",
    "\n",
    "CFG_FILE = \"./utils/config.yaml\"\n",
    "cfg = load_cfg(CFG_FILE)\n",
    "datalake_cfg = cfg[\"datalake\"]\n",
    "crypto_data_cfg = cfg[\"crypto_data\"]\n",
    "\n",
    "# Initialize MinIO client\n",
    "minio_client = Minio(\n",
    "    datalake_cfg['endpoint'],  # Your MinIO server endpoint\n",
    "    access_key=datalake_cfg['access_key'],\n",
    "    secret_key=datalake_cfg['secret_key'],\n",
    "    secure=False  # Set to True if using HTTPS\n",
    ")\n",
    "\n",
    "# Create bucket if it doesn't exist\n",
    "bucket = 'crypto-test'\n",
    "if not minio_client.bucket_exists(bucket):\n",
    "    minio_client.make_bucket(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 17:55:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/03/10 17:55:21 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/03/10 17:55:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the weekly features to Delta on MinIO using streaming write\n",
    "# json_df.write \\\n",
    "#     .format(\"parquet\") \\\n",
    "#     .mode(\"overwrite\")\\\n",
    "#     .save(f\"s3a://{bucket}/delta/test_features\")\n",
    "\n",
    "# from pyspark.sql import Row\n",
    "# test_df = spark.createDataFrame([Row(id=1, value=\"test\")])\n",
    "# test_df.write.format(\"json\").save(\"s3a://crypto-test/test_connection\")\n",
    "json_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(f\"s3a://{bucket}/delta_test/test_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crypto-test'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/15 19:24:36 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/03/15 19:24:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------------------+-------------+---------------+-------------------+-------------------+----------+\n",
      "|    User ID|Transaction ID|            Amount|       Vendor|        Sources|               Time|          timestamp|      date|\n",
      "+-----------+--------------+------------------+-------------+---------------+-------------------+-------------------+----------+\n",
      "|user_000899|  658830851347|  974.933538286743|    Education|Current Account|2025-03-14 16:30:53|2025-03-14 16:30:53|2025-03-14|\n",
      "|user_000719|  743653405057|257.11832424180943|Entertainment|Current Account|2025-03-14 16:12:50|2025-03-14 16:12:50|2025-03-14|\n",
      "|user_000866|  730320152514| 477.4738513432964|        Other|     Debit Card|2025-03-14 19:51:52|2025-03-14 19:51:52|2025-03-14|\n",
      "|user_000977|  620364528242|117.96175593527273|    Education|Current Account|2025-03-14 03:42:49|2025-03-14 03:42:49|2025-03-14|\n",
      "|user_000198|  228177746961| 802.4465173614635|Entertainment|Current Account|2025-03-14 14:53:00|2025-03-14 14:53:00|2025-03-14|\n",
      "|user_000310|  695950861426| 745.9428661651496|    Utilities|    Credit Card|2025-03-14 07:50:52|2025-03-14 07:50:52|2025-03-14|\n",
      "|user_000600|  664716926683| 822.7766457769394|        Other|     Debit Card|2025-03-14 08:39:56|2025-03-14 08:39:56|2025-03-14|\n",
      "|user_000955|  506315032067| 797.6128985032801|    Utilities|Current Account|2025-03-14 20:04:16|2025-03-14 20:04:16|2025-03-14|\n",
      "|user_000294|  892194070028|  461.937545022413|    Education|Current Account|2025-03-14 01:01:44|2025-03-14 01:01:44|2025-03-14|\n",
      "|user_000094|  703108226970| 730.0216607648656|    Education|Current Account|2025-03-14 22:46:35|2025-03-14 22:46:35|2025-03-14|\n",
      "|user_000353|  258730341205| 541.9039961986505|    Education|    Credit Card|2025-03-14 17:41:36|2025-03-14 17:41:36|2025-03-14|\n",
      "|user_000032|  523242568384| 592.2358843715004|Entertainment|    Credit Card|2025-03-14 14:05:43|2025-03-14 14:05:43|2025-03-14|\n",
      "|user_000086|  170367217471| 84.38906018710387|    Education|    Credit Card|2025-03-14 15:44:06|2025-03-14 15:44:06|2025-03-14|\n",
      "|user_000525|  364607317794|151.09649549374325|        Other|     Debit Card|2025-03-14 07:36:58|2025-03-14 07:36:58|2025-03-14|\n",
      "|user_000751|  642368852977| 344.9088202589604|        Other|     Debit Card|2025-03-14 08:32:18|2025-03-14 08:32:18|2025-03-14|\n",
      "|user_000406|  838569564903|475.22093567153894|    Education|Current Account|2025-03-14 22:39:09|2025-03-14 22:39:09|2025-03-14|\n",
      "|user_000397|  900986390801|507.21173783757393|        Other|Current Account|2025-03-14 07:24:00|2025-03-14 07:24:00|2025-03-14|\n",
      "|user_000011|  669297235883| 680.8820250658804|        Other|Current Account|2025-03-14 22:37:55|2025-03-14 22:37:55|2025-03-14|\n",
      "|user_000397|  477917020143| 261.4504802638021|    Education|Current Account|2025-03-14 01:47:04|2025-03-14 01:47:04|2025-03-14|\n",
      "|user_000963|  785237289529| 585.4401378376758|        Sport|Current Account|2025-03-14 03:09:56|2025-03-14 03:09:56|2025-03-14|\n",
      "+-----------+--------------+------------------+-------------+---------------+-------------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df = spark.read.format(\"json\").load(\"s3a://crypto-test/test_connection\")\n",
    "df = spark.read.format(\"delta\").load(f\"s3a://transaction-data/delta_test/\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Available Checkpoints ===\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: s3a://crypto-test/delta_test/test_features/offsets\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: s3a://crypto-test/delta_test/test_features/offsets\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# List checkpoints\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Available Checkpoints ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m batch_ids \u001b[38;5;241m=\u001b[39m \u001b[43mlist_checkpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Get info about the most recent checkpoint\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_ids:\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mlist_checkpoints\u001b[0;34m(checkpoint_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_checkpoints\u001b[39m(checkpoint_dir):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# List all files in the checkpoint directory\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     checkpoint_files \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Extract batch IDs from the files\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     batch_ids \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark/rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: s3a://crypto-test/delta_test/test_features/offsets\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: s3a://crypto-test/delta_test/test_features/offsets\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ====== 1. List all checkpoints for a streaming query ======\n",
    "def list_checkpoints(checkpoint_dir):\n",
    "    # List all files in the checkpoint directory\n",
    "    checkpoint_files = spark.sparkContext.textFile(os.path.join(checkpoint_dir, \"offsets\")).collect()\n",
    "    \n",
    "    # Extract batch IDs from the files\n",
    "    batch_ids = []\n",
    "    for file in checkpoint_files:\n",
    "        if \"batch-\" in file:\n",
    "            batch_id = file.split(\"batch-\")[1].split(\".\")[0]\n",
    "            batch_ids.append(int(batch_id))\n",
    "    \n",
    "    # Sort batch IDs to show most recent first\n",
    "    batch_ids.sort(reverse=True)\n",
    "    \n",
    "    print(f\"Found {len(batch_ids)} checkpoints\")\n",
    "    for i, batch_id in enumerate(batch_ids[:10]):  # Show only the 10 most recent\n",
    "        print(f\"Batch ID: {batch_id}\")\n",
    "    \n",
    "    return batch_ids\n",
    "\n",
    "# ====== 2. Get metadata about a specific checkpoint ======\n",
    "def get_checkpoint_info(checkpoint_dir, batch_id):\n",
    "    # Read the metadata JSON file for this batch\n",
    "    metadata_path = os.path.join(checkpoint_dir, f\"offsets/batch-{batch_id}.json\")\n",
    "    metadata_df = spark.read.json(metadata_path)\n",
    "    \n",
    "    # Display checkpoint details\n",
    "    metadata_df.show(truncate=False)\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "# ====== 3. Find the checkpoint location used by a Delta table ======\n",
    "def get_delta_table_checkpoint(delta_table_path):\n",
    "    # Load the Delta table\n",
    "    delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "    \n",
    "    # Get the transaction log and last checkpoint information\n",
    "    history = delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\")\n",
    "    \n",
    "    # Show recent operations\n",
    "    print(\"Recent Delta table operations:\")\n",
    "    history.orderBy(\"version\", ascending=False).show(5, truncate=False)\n",
    "    \n",
    "    # Find streaming write operations with checkpoint info\n",
    "    streaming_writes = history.filter(\"operation = 'STREAMING UPDATE' OR operation = 'WRITE'\")\n",
    "    \n",
    "    # Extract checkpoint location from operation parameters\n",
    "    if streaming_writes.count() > 0:\n",
    "        # The checkpoint location should be in the operationParameters\n",
    "        checkpoint_info = streaming_writes.first()\n",
    "        \n",
    "        # Parse the JSON operationParameters to extract checkpoint location\n",
    "        params = streaming_writes.select(\"operationParameters\").first()[0]\n",
    "        if \"checkpointLocation\" in params:\n",
    "            print(f\"Checkpoint location: {params['checkpointLocation']}\")\n",
    "            return params['checkpointLocation']\n",
    "        else:\n",
    "            print(\"No checkpoint location found in operation parameters\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No streaming writes found in the Delta table history\")\n",
    "        return None\n",
    "\n",
    "# ====== 4. Execute the functions ======\n",
    "checkpoint_path = f\"s3a://{bucket}/delta_test/test_features\"\n",
    "# List checkpoints\n",
    "print(\"=== Available Checkpoints ===\")\n",
    "batch_ids = list_checkpoints(checkpoint_path)\n",
    "\n",
    "# # Get info about the most recent checkpoint\n",
    "# if batch_ids:\n",
    "#     print(\"\\n=== Most Recent Checkpoint Info ===\")\n",
    "#     get_checkpoint_info(checkpoint_path, batch_ids[0])\n",
    "\n",
    "# # Find checkpoint location used by the Delta table\n",
    "# print(\"\\n=== Delta Table Checkpoint Information ===\")\n",
    "# table_checkpoint = get_delta_table_checkpoint(delta_table_path)\n",
    "\n",
    "# # ====== 5. Bonus: Get commit info from Delta table ======\n",
    "# print(\"\\n=== Delta Table Commit Information ===\")\n",
    "# spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import subprocess\n",
    "\n",
    "# Path to delete\n",
    "path_to_delete = \"s3a://crypto-test/delta/\"\n",
    "\n",
    "def delete_s3_path(path: str):\n",
    "    \"\"\"\n",
    "    Delete a path in S3 using the Hadoop FileSystem API\n",
    "    Args:\n",
    "    \"\"\"\n",
    "    # Get the Hadoop configuration\n",
    "    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "\n",
    "    # Get the FileSystem for S3\n",
    "    fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark.sparkContext._jvm.java.net.URI.create(path), hadoop_conf\n",
    "    )\n",
    "\n",
    "    # Create path object\n",
    "    hadoop_path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(path)\n",
    "\n",
    "    # Delete the path (true means recursive deletion)\n",
    "    if fs.exists(hadoop_path):\n",
    "        fs.delete(hadoop_path, True)\n",
    "        print(f\"Successfully deleted {path}\")\n",
    "    else:\n",
    "        print(f\"Path {path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate weekly features: average price, total volume, and total trades per week\n",
    "# weekly_df = json_df.groupBy(\"symbol\", weekofyear(\"agg_date\").alias(\"week_number\")) \\\n",
    "#     .agg(\n",
    "#         avg(\"avgPrice\").alias(\"weekly_avg_price\"),\n",
    "#         _sum(\"totalVolume\").alias(\"weekly_total_volume\"),\n",
    "#         _sum(\"totalTrades\").alias(\"weekly_total_trades\")\n",
    "#     )\n",
    "\n",
    "# # Similarly, calculate monthly features\n",
    "# monthly_df = json_df.groupBy(\"symbol\", month(\"agg_date\").alias(\"month\")) \\\n",
    "#     .agg(\n",
    "#         avg(\"avgPrice\").alias(\"monthly_avg_price\"),\n",
    "#         _sum(\"totalVolume\").alias(\"monthly_total_volume\"),\n",
    "#         _sum(\"totalTrades\").alias(\"monthly_total_trades\")\n",
    "#     )\n",
    "\n",
    "# # Write the weekly features to Delta on MinIO using streaming write with a checkpoint location\n",
    "# weekly_query = weekly_df.writeStream \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .option(\"checkpointLocation\", \"s3a://your-bucket/delta/weekly_checkpoint\") \\\n",
    "#     .start(\"s3a://your-bucket/delta/weekly_features\")\n",
    "\n",
    "# # Write the monthly features to Delta on MinIO using streaming write with a checkpoint location\n",
    "# monthly_query = monthly_df.writeStream \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .option(\"checkpointLocation\", \"s3a://your-bucket/delta/monthly_checkpoint\") \\\n",
    "#     .start(\"s3a://your-bucket/delta/monthly_features\")\n",
    "\n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = os.path.join(\"./scripts/\", '.env')  # Assuming .env is in the same directory\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Debezium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted 20000 rows into transaction_data table at 2025-03-02\n",
      "Table contains 20000 rows\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, types, MetaData, Table, Column, String, Float, text, Integer, Identity\n",
    "\n",
    "# Expect a run date (e.g., \"2025-03-20\") as the first command-line argument\n",
    "RUN_DATE_STR = \"2025-03-02\"\n",
    "\n",
    "def generate_pseudo_data(run_date_str, num_rows=20000, num_users=1000):\n",
    "    \"\"\"Generates a Pandas DataFrame with pseudo transaction data.\"\"\"\n",
    "    # Generate pseudo user IDs\n",
    "    user_ids = [f\"user_{i:06d}\" for i in np.random.choice(num_users, size=num_rows)]\n",
    "    \n",
    "    # Generate pseudo transaction IDs\n",
    "    transaction_ids = np.random.randint(10**11, 10**12 - 1, size=num_rows)\n",
    "    \n",
    "    # Choose sources for the transactions\n",
    "    sources = np.random.choice([\"Current Account\", \"Credit Card\", \"Debit Card\"],\n",
    "                              size=num_rows, p=[0.6, 0.3, 0.1])\n",
    "    \n",
    "    # Generate random amounts\n",
    "    amounts = np.random.uniform(1.0, 1000.0, size=num_rows)\n",
    "    \n",
    "    # Choose vendors from a list, with random probabilities\n",
    "    vendors = [\"Online Shopping\", \"Hospital\", \"Sport\", \"Grocery\", \"Restaurant\",\n",
    "               \"Travel\", \"Entertainment\", \"Electronics\", \"Home Improvement\",\n",
    "               \"Clothing\", \"Education\", \"Sending Out\", \"Utilities\", \"Other\"]\n",
    "    vendor_probabilities = np.random.dirichlet(np.ones(len(vendors)))\n",
    "    vendor_choices = np.random.choice(vendors, size=num_rows, p=vendor_probabilities)\n",
    "    \n",
    "    # Generate pseudo times based on the run date\n",
    "    start_date = dt.datetime.strptime(run_date_str, \"%Y-%m-%d\")\n",
    "    time_deltas = [dt.timedelta(seconds=np.random.randint(0, 31536000)) for _ in range(num_rows)]\n",
    "    times = [(start_date + delta).strftime('%Y-%m-%d %H:%M:%S') for delta in time_deltas]\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"User ID\": user_ids,\n",
    "        \"Transaction ID\": transaction_ids,\n",
    "        \"Amount\": amounts,\n",
    "        \"Vendor\": vendor_choices,\n",
    "        \"Sources\": sources,\n",
    "        \"Time\": times\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def insert_data_to_postgres(df, table_name=\"transaction_data\"):\n",
    "    \"\"\"\n",
    "    Inserts the DataFrame into a PostgreSQL table.\n",
    "    It uses SQLAlchemy to connect to PostgreSQL.\n",
    "    \"\"\"\n",
    "    # Use connection string from environment or default to the Airflow connection string\n",
    "    postgres_conn_str = \"postgresql+psycopg2://airflow:airflow@localhost/airflow\"\n",
    "    engine = create_engine(postgres_conn_str)\n",
    "    \n",
    "    try:\n",
    "        # Define table metadata with primary key\n",
    "        metadata = MetaData()\n",
    "        transaction_table = Table(\n",
    "            table_name, metadata,\n",
    "            Column(\"id\", Integer, Identity(), primary_key=True),  # Auto-incrementing primary key\n",
    "            Column(\"User ID\", String(255)),\n",
    "            Column(\"Transaction ID\", String(255)),\n",
    "            Column(\"Amount\", Float),\n",
    "            Column(\"Vendor\", String(255)),\n",
    "            Column(\"Sources\", String(255)),\n",
    "            Column(\"Time\", String(255)),\n",
    "            schema='public'\n",
    "        )\n",
    "        \n",
    "        # Create the table in the database\n",
    "        metadata.create_all(engine)\n",
    "        \n",
    "        # Insert data into the table using pandas to_sql\n",
    "        df.to_sql(\n",
    "            name=table_name,\n",
    "            con=engine,\n",
    "            if_exists='append',  # Use 'append' to add data to the existing table\n",
    "            index=False,\n",
    "            schema='public'\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully inserted {len(df)} rows into {table_name} table at {RUN_DATE_STR}\")\n",
    "\n",
    "        # Verify the table exists\n",
    "        with engine.connect() as connection:\n",
    "            query = text(f\"SELECT COUNT(*) FROM public.{table_name}\")\n",
    "            result = connection.execute(query)\n",
    "            count = result.scalar()\n",
    "            print(f\"Table contains {count} rows\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate pseudo transaction data\n",
    "    df = generate_pseudo_data(RUN_DATE_STR)\n",
    "    \n",
    "    # Insert the pseudo data into PostgreSQL\n",
    "    insert_data_to_postgres(df, table_name=\"transaction_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "def check_connector_exists():\n",
    "    try:\n",
    "        response = requests.get('http://localhost:8083/connectors/postgres-connector')\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return False\n",
    "\n",
    "def check_kafka_connect_health():\n",
    "    try:\n",
    "        response = requests.get('http://localhost:8083')\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "def register_connector():\n",
    "    # Wait for Kafka Connect to be ready\n",
    "    retries = 30\n",
    "    while retries > 0:\n",
    "        if check_kafka_connect_health():\n",
    "            break\n",
    "        print(\"Waiting for Kafka Connect to be ready...\")\n",
    "        time.sleep(5)\n",
    "        retries -= 1\n",
    "    \n",
    "    if retries == 0:\n",
    "        raise Exception(\"Kafka Connect is not available after waiting\")\n",
    "    \n",
    "    # Check if connector already exists\n",
    "    if check_connector_exists():\n",
    "        print(\"Connector already registered, checking status...\")\n",
    "        response = requests.get('http://localhost:8083/connectors/postgres-connector/status')\n",
    "        print(f\"Connector status: {response.json()}\")\n",
    "        sys.exit(0)\n",
    "    \n",
    "    # Load connector configuration\n",
    "    config_path = './docker_all/config/config_debezium.json'\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found at {config_path}\")\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        connector_config = json.load(f)\n",
    "    \n",
    "    try:\n",
    "        # Register connector\n",
    "        response = requests.post(\n",
    "            'http://localhost:8083/connectors',\n",
    "            headers={'Content-Type': 'application/json'},\n",
    "            data=json.dumps(connector_config)\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 201:\n",
    "            print(\"Connector registered successfully\")\n",
    "            # Check connector status\n",
    "            time.sleep(2)  # Wait for connector to start\n",
    "            status_response = requests.get('http://localhost:8083/connectors/postgres-connector/status')\n",
    "            print(f\"Connector status: {status_response.json()}\")\n",
    "        else:\n",
    "            print(f\"Failed to register connector: {response.text}\")\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"Failed to connect to Kafka Connect: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    register_connector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_connector_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get run date from command line argument\n",
    "RUN_DATE_STR = sys.argv[1]\n",
    "RUN_DATE = dt.datetime.strptime(RUN_DATE_STR, \"%Y-%m-%d\")\n",
    "RUN_DATE_STR_7DAYS = (RUN_DATE - dt.timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Environment variables\n",
    "KAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\", \"kafka:29092\")\n",
    "CDC_TOPIC = \"dbserver1.airflow.transaction_data\"  # Adjust if your topic name is different\n",
    "POSTGRES_CONN_STR = os.getenv(\"POSTGRES_CONN_STR\", \"jdbc:postgresql://postgres:5432/airflow\")\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\", \"airflow\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\", \"airflow\")\n",
    "MINIO_ENDPOINT = os.getenv(\"S3_ENDPOINT\", \"minio:9000\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\", \"minio_access_key\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\", \"minio_secret_key\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"features\")\n",
    "\n",
    "def main():\n",
    "    logger.info(f\"Starting data processing for date range: {RUN_DATE_STR_7DAYS} to {RUN_DATE_STR}\")\n",
    "    \n",
    "    # Create Spark session configured for MinIO (S3A)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Combined_CDC_PostgreSQL_Processing\") \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "                \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.2,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.261,\"\n",
    "                \"org.postgresql:postgresql:42.5.1\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{MINIO_ENDPOINT}\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # 1. Process CDC events from Kafka\n",
    "    cdc_df = process_cdc_events(spark)\n",
    "    \n",
    "    # 2. Query PostgreSQL data directly (for data that might not have CDC events yet)\n",
    "    postgres_df = query_postgres_data(spark)\n",
    "    \n",
    "    # 3. Combine the data (union the dataframes)\n",
    "    combined_df = combine_data(cdc_df, postgres_df)\n",
    "    \n",
    "    # 4. Calculate features\n",
    "    features_df = calculate_features(combined_df)\n",
    "    \n",
    "    # 5. Write results to MinIO using Delta format\n",
    "    write_to_minio(features_df)\n",
    "    \n",
    "    spark.stop()\n",
    "    logger.info(\"Processing completed successfully\")\n",
    "\n",
    "def process_cdc_events(spark):\n",
    "    \"\"\"Process CDC events from Kafka\"\"\"\n",
    "    logger.info(f\"Reading CDC events from Kafka topic: {CDC_TOPIC}\")\n",
    "    \n",
    "    # Read data from Kafka CDC topic\n",
    "    raw_kafka_df = spark.read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "        .option(\"subscribe\", CDC_TOPIC) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Define schema for Debezium CDC events\n",
    "    # Note: This schema is specific to Debezium's PostgreSQL connector output format\n",
    "    cdc_schema = StructType([\n",
    "        StructField(\"schema\", StringType(), True),\n",
    "        StructField(\"payload\", StructType([\n",
    "            StructField(\"after\", StringType(), True),  # Data after change\n",
    "            StructField(\"before\", StringType(), True), # Data before change (for updates/deletes)\n",
    "            StructField(\"op\", StringType(), True),     # Operation type (c=create, u=update, d=delete)\n",
    "            StructField(\"ts_ms\", DoubleType(), True)   # Timestamp of the change\n",
    "        ]), True)\n",
    "    ])\n",
    "    \n",
    "    # Parse the value column (which contains the CDC event JSON)\n",
    "    parsed_df = raw_kafka_df \\\n",
    "        .selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "        .select(F.from_json(F.col(\"json_value\"), cdc_schema).alias(\"cdc\")) \\\n",
    "        .select(\"cdc.payload.*\")\n",
    "    \n",
    "    # Define schema for the transaction data\n",
    "    transaction_schema = StructType([\n",
    "        StructField(\"User ID\", StringType(), True),\n",
    "        StructField(\"Transaction ID\", StringType(), True),\n",
    "        StructField(\"Amount\", DoubleType(), True),\n",
    "        StructField(\"Vendor\", StringType(), True),\n",
    "        StructField(\"Sources\", StringType(), True),\n",
    "        StructField(\"Time\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Extract and parse the 'after' data (which contains the actual transaction data)\n",
    "    # Filter to only include 'create' and 'update' operations (skip deletes)\n",
    "    transactions_df = parsed_df \\\n",
    "        .where(\"op IN ('c', 'u')\") \\\n",
    "        .select(\n",
    "            F.from_json(F.col(\"after\"), transaction_schema).alias(\"data\"),\n",
    "            F.col(\"ts_ms\").alias(\"cdc_timestamp\")\n",
    "        ) \\\n",
    "        .select(\"data.*\", \"cdc_timestamp\")\n",
    "    \n",
    "    # Convert timestamp string to proper timestamp and add date column\n",
    "    result_df = transactions_df \\\n",
    "        .withColumn(\"timestamp\", F.to_timestamp(F.col(\"Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "        .withColumn(\"date\", F.to_date(F.col(\"timestamp\"))) \\\n",
    "        .where(f\"date BETWEEN '{RUN_DATE_STR_7DAYS}' AND '{RUN_DATE_STR}'\") \\\n",
    "        .drop(\"cdc_timestamp\")  # Remove the CDC timestamp as we don't need it anymore\n",
    "    \n",
    "    logger.info(f\"Processed {result_df.count()} CDC events\")\n",
    "    return result_df\n",
    "\n",
    "def query_postgres_data(spark):\n",
    "    \"\"\"Query PostgreSQL data directly\"\"\"\n",
    "    logger.info(\"Querying PostgreSQL database directly\")\n",
    "    \n",
    "    # Read directly from PostgreSQL for recent data that might not be in CDC yet\n",
    "    postgres_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", POSTGRES_CONN_STR) \\\n",
    "        .option(\"dbtable\", \"transaction_data\") \\\n",
    "        .option(\"user\", POSTGRES_USER) \\\n",
    "        .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Convert timestamp and filter by date range\n",
    "    result_df = postgres_df \\\n",
    "        .withColumn(\"timestamp\", F.to_timestamp(F.col(\"Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "        .withColumn(\"date\", F.to_date(F.col(\"timestamp\"))) \\\n",
    "        .where(f\"date BETWEEN '{RUN_DATE_STR_7DAYS}' AND '{RUN_DATE_STR}'\")\n",
    "    \n",
    "    logger.info(f\"Retrieved {result_df.count()} records from PostgreSQL\")\n",
    "    return result_df\n",
    "\n",
    "def combine_data(cdc_df, postgres_df):\n",
    "    \"\"\"Combine CDC events and PostgreSQL data, removing duplicates\"\"\"\n",
    "    logger.info(\"Combining CDC events and PostgreSQL data\")\n",
    "    \n",
    "    # Union the dataframes\n",
    "    combined_df = cdc_df.unionByName(postgres_df)\n",
    "    \n",
    "    # Remove duplicates based on Transaction ID (keeping the latest record)\n",
    "    deduplicated_df = combined_df \\\n",
    "        .withColumn(\"row_num\", F.row_number().over(\n",
    "            F.window.partitionBy(\"Transaction ID\").orderBy(F.desc(\"timestamp\"))\n",
    "        )) \\\n",
    "        .where(\"row_num = 1\") \\\n",
    "        .drop(\"row_num\")\n",
    "    \n",
    "    logger.info(f\"Combined data has {deduplicated_df.count()} unique transactions after deduplication\")\n",
    "    return deduplicated_df\n",
    "\n",
    "def calculate_features(df):\n",
    "    \"\"\"Calculate features from the transaction data\"\"\"\n",
    "    logger.info(\"Calculating features from transaction data\")\n",
    "    \n",
    "    # Calculate weekly features (l1w = last 1 week)\n",
    "    time_window = \"l1w\"\n",
    "    features_df = df.groupBy(\"User ID\") \\\n",
    "        .agg(\n",
    "            F.count(\"Transaction ID\").alias(f\"num_transactions_{time_window}\"),\n",
    "            F.sum(\"Amount\").alias(f\"total_amount_{time_window}\"),\n",
    "            F.avg(\"Amount\").alias(f\"avg_amount_{time_window}\"),\n",
    "            F.min(\"Amount\").alias(f\"min_amount_{time_window}\"),\n",
    "            F.max(\"Amount\").alias(f\"max_amount_{time_window}\"),\n",
    "            F.countDistinct(\"Vendor\").alias(f\"num_vendors_{time_window}\"),\n",
    "            F.countDistinct(\"Sources\").alias(f\"num_sources_{time_window}\")\n",
    "        )\n",
    "    \n",
    "    # Add a timestamp for when these features were calculated\n",
    "    features_df = features_df.withColumn(\"feature_timestamp\", F.current_timestamp())\n",
    "    \n",
    "    logger.info(f\"Calculated features for {features_df.count()} users\")\n",
    "    return features_df\n",
    "\n",
    "def write_to_minio(df):\n",
    "    \"\"\"Write features to MinIO using Delta format\"\"\"\n",
    "    logger.info(f\"Writing features to MinIO bucket: {MINIO_BUCKET}\")\n",
    "    \n",
    "    # Ensure MinIO bucket exists\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT,\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    if not minio_client.bucket_exists(MINIO_BUCKET):\n",
    "        minio_client.make_bucket(MINIO_BUCKET)\n",
    "        logger.info(f\"Created new bucket: {MINIO_BUCKET}\")\n",
    "    \n",
    "    # Write to MinIO using Delta format\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "        .option(\"delta.minReaderVersion\", \"2\") \\\n",
    "        .option(\"delta.minWriterVersion\", \"5\") \\\n",
    "        .save(f\"s3a://{MINIO_BUCKET}/features/date={RUN_DATE_STR}\")\n",
    "    \n",
    "    logger.info(f\"Successfully wrote features to s3a://{MINIO_BUCKET}/features/date={RUN_DATE_STR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "\n",
    "# Get run date from command line argument\n",
    "# RUN_DATE_STR = sys.argv[1]\n",
    "# RUN_DATE = dt.datetime.strptime(RUN_DATE_STR, \"%Y-%m-%d\")\n",
    "# RUN_DATE_STR_7DAYS = (RUN_DATE - dt.timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "dotenv_path = os.path.join(\"./scripts/\", '.env') \n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Environment variables\n",
    "KAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\n",
    "CDC_TRANSACTION_TOPIC = os.getenv(\"CDC_TRANSACTION_TOPIC\")\n",
    "POSTGRES_CON_STR = os.getenv(\"POSTGRES_CON_STR\")\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "MINIO_ENDPOINT = os.getenv(\"S3_ENDPOINT\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\")\n",
    "\n",
    "    \n",
    "# Create Spark session configured for MinIO (S3A)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Combined_CDC_PostgreSQL_Processing\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.2,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.261,\"\n",
    "            \"org.postgresql:postgresql:42.5.1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{MINIO_ENDPOINT}\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 18:11:32 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+---+-------------+-----------+\n",
      "|before|               after|              source| op|        ts_ms|transaction|\n",
      "+------+--------------------+--------------------+---+-------------+-----------+\n",
      "|  null|{1, user_000377, ...|{2.3.4.Final, pos...|  r|1742498526222|       null|\n",
      "|  null|{2, user_000981, ...|{2.3.4.Final, pos...|  r|1742498526233|       null|\n",
      "|  null|{3, user_000836, ...|{2.3.4.Final, pos...|  r|1742498526234|       null|\n",
      "|  null|{4, user_000962, ...|{2.3.4.Final, pos...|  r|1742498526234|       null|\n",
      "|  null|{5, user_000640, ...|{2.3.4.Final, pos...|  r|1742498526234|       null|\n",
      "+------+--------------------+--------------------+---+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 18:11:33 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+--------------+-----------------+-----------+---------------+-------------------+-------------+\n",
      "| op| id|    User ID|Transaction ID|           Amount|     Vendor|        Sources|               Time|        ts_ms|\n",
      "+---+---+-----------+--------------+-----------------+-----------+---------------+-------------------+-------------+\n",
      "|  u|  4|user_000962|  457239087380| 174.213256648849|Sending Out|Current Account|2025-06-09 18:27:06|1742578440256|\n",
      "|  u| 48|user_000468|  155352729109|515.6645561652284|Sending Out|Current Account|2025-03-27 00:14:38|1742578440256|\n",
      "|  u| 51|user_000303|  732970514123| 950.034224384725|Sending Out|Current Account|2025-04-20 09:00:18|1742578440256|\n",
      "|  u| 81|user_000013|  863604161233| 868.166665533098|Sending Out|Current Account|2025-04-09 09:08:31|1742578440256|\n",
      "|  u| 85|user_000324|  619489722520| 317.590035743889|Sending Out|Current Account|2026-01-01 16:36:22|1742578440256|\n",
      "+---+---+-----------+--------------+-----------------+-----------+---------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_kafka_df = spark.read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", CDC_TRANSACTION_TOPIC) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "# Define the nested schema for transaction data fields\n",
    "transaction_fields = [\n",
    "    T.StructField(\"id\", T.IntegerType(), False),\n",
    "    T.StructField(\"User ID\", T.StringType(), True),\n",
    "    T.StructField(\"Transaction ID\", T.StringType(), True),\n",
    "    T.StructField(\"Amount\", T.DoubleType(), True),\n",
    "    T.StructField(\"Vendor\", T.StringType(), True),\n",
    "    T.StructField(\"Sources\", T.StringType(), True),\n",
    "    T.StructField(\"Time\", T.StringType(), True)\n",
    "]\n",
    "\n",
    "# Define the nested schema for source\n",
    "source_fields = [\n",
    "    T.StructField(\"version\", T.StringType(), False),\n",
    "    T.StructField(\"connector\", T.StringType(), False),\n",
    "    T.StructField(\"name\", T.StringType(), False),\n",
    "    T.StructField(\"ts_ms\", T.LongType(), False),\n",
    "    T.StructField(\"snapshot\", T.StringType(), True),\n",
    "    T.StructField(\"db\", T.StringType(), False),\n",
    "    T.StructField(\"sequence\", T.StringType(), True),\n",
    "    T.StructField(\"schema\", T.StringType(), False),\n",
    "    T.StructField(\"table\", T.StringType(), False),\n",
    "    T.StructField(\"txId\", T.LongType(), True),\n",
    "    T.StructField(\"lsn\", T.LongType(), True),\n",
    "    T.StructField(\"xmin\", T.LongType(), True)\n",
    "]\n",
    "\n",
    "# Define the schema for transaction block\n",
    "transaction_block_fields = [\n",
    "    T.StructField(\"id\", T.StringType(), False),\n",
    "    T.StructField(\"total_order\", T.LongType(), False),\n",
    "    T.StructField(\"data_collection_order\", T.LongType(), False)\n",
    "]\n",
    "\n",
    "# Complete CDC schema\n",
    "cdc_schema = T.StructType([\n",
    "    T.StructField(\"schema\", T.StructType([\n",
    "        T.StructField(\"type\", T.StringType()),\n",
    "        T.StructField(\"fields\", T.ArrayType(T.StringType())),\n",
    "        T.StructField(\"optional\", T.BooleanType()),\n",
    "        T.StructField(\"name\", T.StringType()),\n",
    "        T.StructField(\"version\", T.IntegerType())\n",
    "    ])),\n",
    "    T.StructField(\"payload\", T.StructType([\n",
    "        T.StructField(\"before\", T.StructType(transaction_fields), True),\n",
    "        T.StructField(\"after\", T.StructType(transaction_fields), True), \n",
    "        T.StructField(\"source\", T.StructType(source_fields), False),\n",
    "        T.StructField(\"op\", T.StringType(), False),\n",
    "        T.StructField(\"ts_ms\", T.LongType(), True),\n",
    "        T.StructField(\"transaction\", T.StructType(transaction_block_fields), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Parse the JSON data\n",
    "cdc_df = raw_kafka_df \\\n",
    "  .selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "  .withColumn(\"parsed_data\", F.from_json(F.col(\"json_str\"), cdc_schema)) \\\n",
    "  .select(\"parsed_data.payload.*\")\n",
    "\n",
    "# Now show the parsed data\n",
    "cdc_df.show(5)\n",
    "\n",
    "# To get just the transaction data after the change\n",
    "transaction_data = cdc_df.where('''op in ('c','u')''').select(\n",
    "    \"op\", \n",
    "    \"after.id\", \n",
    "    \"after.`User ID`\", \n",
    "    \"after.`Transaction ID`\", \n",
    "    \"after.Amount\", \n",
    "    \"after.Vendor\", \n",
    "    \"after.Sources\", \n",
    "    \"after.Time\",\n",
    "    \"source.ts_ms\"\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id\").orderBy(F.col(\"ts_ms\").desc())\n",
    "transaction_data = transaction_data.withColumn(\"rank\", F.row_number().over(w)) \\\n",
    "                        .filter(F.col(\"rank\") == 1) \\\n",
    "                        .drop(\"rank\")\n",
    "\n",
    "transaction_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", POSTGRES_CON_STR) \\\n",
    "    .option(\"dbtable\", \"transaction_data\") \\\n",
    "    .option(\"user\", POSTGRES_USER) \\\n",
    "    .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "postgres_df = postgres_df.withColumn(\"timestamp\", F.to_timestamp(F.col(\"Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                        .withColumn(\"date\", F.to_date(F.col(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+-----------------+---------------+---------------+-------------------+-------------------+----------+\n",
      "| id|    User ID|Transaction ID|           Amount|         Vendor|        Sources|               Time|          timestamp|      date|\n",
      "+---+-----------+--------------+-----------------+---------------+---------------+-------------------+-------------------+----------+\n",
      "|  1|user_000377|  989600621541|458.3600134428063|      Education|    Credit Card|2025-05-27 01:14:33|2025-05-27 01:14:33|2025-05-27|\n",
      "|  2|user_000981|  344360205463|38.14437463217147|      Education|    Credit Card|2025-07-30 11:36:25|2025-07-30 11:36:25|2025-07-30|\n",
      "|  3|user_000836|  754588743418|988.4897927900346|      Education|Current Account|2025-04-21 18:01:59|2025-04-21 18:01:59|2025-04-21|\n",
      "|  5|user_000640|  413270206972|78.78715816071131|Online Shopping|    Credit Card|2025-03-24 10:52:26|2025-03-24 10:52:26|2025-03-24|\n",
      "|  6|user_000205|  754963731286|943.7920762607532|  Entertainment|Current Account|2025-11-21 13:54:16|2025-11-21 13:54:16|2025-11-21|\n",
      "+---+-----------+--------------+-----------------+---------------+---------------+-------------------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postgres_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
